{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baacc14e",
   "metadata": {},
   "source": [
    "# AFT Contact Data - Incremental Cleaning Notebook\n",
    "\n",
    "This notebook provides a safe, incremental approach to cleaning the AFT_RR_GRP1.csv contact data.\n",
    "Each section focuses on specific columns with before/after comparisons to prevent data loss.\n",
    "\n",
    "**Safety Features:**\n",
    "- Process one column or related column set at a time\n",
    "- Visual before/after comparisons\n",
    "- Backup original data\n",
    "- Step-by-step validation\n",
    "- Option to skip or modify cleaning rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8a53328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "\u001b[?25lInstalling collected packages: pytz, tzdata, numpy, pandas\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/4\u001b[0m [pandas]2m3/4\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.2.6 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4/4\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.2.6 pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.2.6)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85eba7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display options for better viewing\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250021bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset and Create Backup\n",
    "df_original = pd.read_csv('AFT_RR_GRP1.csv')\n",
    "df_working = df_original.copy()  # Working copy for cleaning\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Total records: {len(df_original)}\")\n",
    "print(f\"Total columns: {len(df_original.columns)}\")\n",
    "print(\"\\nColumn names:\")\n",
    "for i, col in enumerate(df_original.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389cf349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Data Inspection\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Shape: {df_original.shape}\")\n",
    "print(f\"\\nData types:\")\n",
    "print(df_original.dtypes)\n",
    "\n",
    "print(\"\\n=== MISSING VALUES BY COLUMN ===\")\n",
    "missing_data = df_original.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "if len(missing_data) > 0:\n",
    "    print(missing_data)\n",
    "else:\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "print(\"\\n=== FIRST 3 ROWS ===\")\n",
    "df_original.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aeda9b",
   "metadata": {},
   "source": [
    "## Section 1: Email Cleaning (Email_1)\n",
    "\n",
    "First, let's examine and clean the primary email column. We'll look for:\n",
    "- Trailing semicolons\n",
    "- Invalid email formats\n",
    "- Duplicate emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eedcf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Email_1 column\n",
    "print(\"=== EMAIL_1 COLUMN ANALYSIS ===\")\n",
    "print(f\"Total entries: {len(df_working)}\")\n",
    "print(f\"Non-null entries: {df_working['Email_1'].notna().sum()}\")\n",
    "print(f\"Unique entries: {df_working['Email_1'].nunique()}\")\n",
    "\n",
    "print(\"\\n=== SAMPLE EMAIL_1 VALUES ===\")\n",
    "print(df_working['Email_1'].head(10).tolist())\n",
    "\n",
    "print(\"\\n=== EMAILS WITH POTENTIAL ISSUES ===\")\n",
    "# Check for trailing semicolons\n",
    "emails_with_semicolon = df_working[df_working['Email_1'].str.endswith(';', na=False)]\n",
    "print(f\"Emails ending with semicolon: {len(emails_with_semicolon)}\")\n",
    "if len(emails_with_semicolon) > 0:\n",
    "    print(\"Examples:\")\n",
    "    print(emails_with_semicolon[['First Name', 'Last name', 'Email_1']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf79750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email Cleaning Function\n",
    "def clean_email(email):\n",
    "    \"\"\"Clean email address by removing trailing semicolons and validating format\"\"\"\n",
    "    if pd.isna(email) or email == '':\n",
    "        return email\n",
    "    \n",
    "    # Remove trailing semicolon\n",
    "    cleaned = str(email).strip().rstrip(';')\n",
    "    \n",
    "    # Basic email validation pattern\n",
    "    email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    \n",
    "    if re.match(email_pattern, cleaned):\n",
    "        return cleaned\n",
    "    else:\n",
    "        # Return original if doesn't match pattern (manual review needed)\n",
    "        return email\n",
    "\n",
    "# Apply cleaning to Email_1\n",
    "df_working['Email_1_cleaned'] = df_working['Email_1'].apply(clean_email)\n",
    "\n",
    "print(\"Email cleaning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c0b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE/AFTER COMPARISON - Email_1\n",
    "print(\"=== EMAIL_1 BEFORE/AFTER COMPARISON ===\")\n",
    "\n",
    "# Show changes\n",
    "changes_mask = df_working['Email_1'] != df_working['Email_1_cleaned']\n",
    "changes_df = df_working[changes_mask][['First Name', 'Last name', 'Email_1', 'Email_1_cleaned']]\n",
    "\n",
    "print(f\"Total changes made: {len(changes_df)}\")\n",
    "if len(changes_df) > 0:\n",
    "    print(\"\\nChanges made:\")\n",
    "    print(changes_df)\n",
    "else:\n",
    "    print(\"No changes needed.\")\n",
    "\n",
    "# Validation check\n",
    "print(\"\\n=== VALIDATION ===\")\n",
    "print(f\"Original emails with semicolons: {df_working['Email_1'].str.endswith(';', na=False).sum()}\")\n",
    "print(f\"Cleaned emails with semicolons: {df_working['Email_1_cleaned'].str.endswith(';', na=False).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867d42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY EMAIL_1 CHANGES (Only run this cell if you're satisfied with the changes above)\n",
    "apply_email_changes = input(\"Apply Email_1 cleaning changes? (yes/no): \").lower().strip()\n",
    "\n",
    "if apply_email_changes == 'yes':\n",
    "    df_working['Email_1'] = df_working['Email_1_cleaned']\n",
    "    df_working.drop('Email_1_cleaned', axis=1, inplace=True)\n",
    "    print(\"‚úÖ Email_1 changes applied successfully!\")\n",
    "else:\n",
    "    df_working.drop('Email_1_cleaned', axis=1, inplace=True)\n",
    "    print(\"‚ùå Email_1 changes discarded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea5145f",
   "metadata": {},
   "source": [
    "## Section 2: Phone Number Cleaning (Cellphone Number)\n",
    "\n",
    "Now let's clean the phone numbers by:\n",
    "- Standardizing format\n",
    "- Removing placeholder values like '--'\n",
    "- Validating phone number patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9405dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Cellphone Number column\n",
    "print(\"=== CELLPHONE NUMBER ANALYSIS ===\")\n",
    "print(f\"Total entries: {len(df_working)}\")\n",
    "print(f\"Non-null entries: {df_working['Cellphone Number'].notna().sum()}\")\n",
    "print(f\"Unique entries: {df_working['Cellphone Number'].nunique()}\")\n",
    "\n",
    "print(\"\\n=== SAMPLE CELLPHONE VALUES ===\")\n",
    "unique_phones = df_working['Cellphone Number'].dropna().unique()[:15]\n",
    "for phone in unique_phones:\n",
    "    print(f\"'{phone}'\")\n",
    "\n",
    "print(\"\\n=== PHONE NUMBER PATTERNS ===\")\n",
    "phone_counts = df_working['Cellphone Number'].value_counts().head(10)\n",
    "print(phone_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398bea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phone Number Cleaning Function\n",
    "def clean_phone(phone):\n",
    "    \"\"\"Clean phone number by standardizing format and removing placeholders\"\"\"\n",
    "    if pd.isna(phone) or phone == '' or phone == '--':\n",
    "        return None\n",
    "    \n",
    "    # Convert to string and remove all non-digits\n",
    "    phone_str = str(phone).strip()\n",
    "    digits_only = re.sub(r'\\D', '', phone_str)\n",
    "    \n",
    "    # Handle different length patterns\n",
    "    if len(digits_only) == 10:\n",
    "        # Format as (XXX) XXX-XXXX\n",
    "        return f\"({digits_only[:3]}) {digits_only[3:6]}-{digits_only[6:]}\"\n",
    "    elif len(digits_only) == 11 and digits_only.startswith('1'):\n",
    "        # Remove leading 1 and format\n",
    "        digits_only = digits_only[1:]\n",
    "        return f\"({digits_only[:3]}) {digits_only[3:6]}-{digits_only[6:]}\"\n",
    "    elif len(digits_only) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # Return original if unusual format (for manual review)\n",
    "        return phone_str\n",
    "\n",
    "# Apply cleaning to Cellphone Number\n",
    "df_working['Cellphone_cleaned'] = df_working['Cellphone Number'].apply(clean_phone)\n",
    "\n",
    "print(\"Phone number cleaning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece05453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE/AFTER COMPARISON - Cellphone Number\n",
    "print(\"=== CELLPHONE NUMBER BEFORE/AFTER COMPARISON ===\")\n",
    "\n",
    "# Show changes\n",
    "changes_mask = df_working['Cellphone Number'].astype(str) != df_working['Cellphone_cleaned'].astype(str)\n",
    "changes_df = df_working[changes_mask][['First Name', 'Last name', 'Cellphone Number', 'Cellphone_cleaned']]\n",
    "\n",
    "print(f\"Total changes made: {len(changes_df)}\")\n",
    "if len(changes_df) > 0:\n",
    "    print(\"\\nFirst 15 changes:\")\n",
    "    print(changes_df.head(15))\n",
    "else:\n",
    "    print(\"No changes needed.\")\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\n=== STATISTICS ===\")\n",
    "print(f\"Original non-null phones: {df_working['Cellphone Number'].notna().sum()}\")\n",
    "print(f\"Cleaned non-null phones: {df_working['Cellphone_cleaned'].notna().sum()}\")\n",
    "print(f\"Placeholder '--' removed: {(df_working['Cellphone Number'] == '--').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY CELLPHONE CHANGES (Only run this cell if you're satisfied with the changes above)\n",
    "apply_phone_changes = input(\"Apply Cellphone Number cleaning changes? (yes/no): \").lower().strip()\n",
    "\n",
    "if apply_phone_changes == 'yes':\n",
    "    df_working['Cellphone Number'] = df_working['Cellphone_cleaned']\n",
    "    df_working.drop('Cellphone_cleaned', axis=1, inplace=True)\n",
    "    print(\"‚úÖ Cellphone Number changes applied successfully!\")\n",
    "else:\n",
    "    df_working.drop('Cellphone_cleaned', axis=1, inplace=True)\n",
    "    print(\"‚ùå Cellphone Number changes discarded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83428c46",
   "metadata": {},
   "source": [
    "## Section 3: Social Media Handles - Twitter\n",
    "\n",
    "Let's clean the Twitter Handle column by:\n",
    "- Ensuring handles start with @\n",
    "- Removing invalid entries\n",
    "- Handling multiple handles in one field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Twitter Handle column\n",
    "print(\"=== TWITTER HANDLE ANALYSIS ===\")\n",
    "print(f\"Total entries: {len(df_working)}\")\n",
    "print(f\"Non-null entries: {df_working['Twitter Handle'].notna().sum()}\")\n",
    "print(f\"Unique entries: {df_working['Twitter Handle'].nunique()}\")\n",
    "\n",
    "print(\"\\n=== SAMPLE TWITTER HANDLES ===\")\n",
    "twitter_handles = df_working['Twitter Handle'].dropna().unique()[:20]\n",
    "for handle in twitter_handles:\n",
    "    print(f\"'{handle}'\")\n",
    "\n",
    "print(\"\\n=== HANDLES WITH MULTIPLE VALUES ===\")\n",
    "multi_handles = df_working[df_working['Twitter Handle'].str.contains(',', na=False)]\n",
    "if len(multi_handles) > 0:\n",
    "    print(f\"Found {len(multi_handles)} entries with multiple handles:\")\n",
    "    print(multi_handles[['First Name', 'Last name', 'Twitter Handle']].head())\n",
    "else:\n",
    "    print(\"No entries with multiple handles found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Handle Cleaning Function\n",
    "def clean_twitter_handle(handle):\n",
    "    \"\"\"Clean Twitter handle by ensuring @ prefix and handling multiple values\"\"\"\n",
    "    if pd.isna(handle) or handle == '' or handle == '--':\n",
    "        return None\n",
    "    \n",
    "    handle_str = str(handle).strip()\n",
    "    \n",
    "    # Handle multiple comma-separated handles\n",
    "    if ',' in handle_str:\n",
    "        handles = [h.strip() for h in handle_str.split(',')]\n",
    "        cleaned_handles = []\n",
    "        \n",
    "        for h in handles:\n",
    "            if h and h != '--':\n",
    "                # Add @ if not present\n",
    "                if not h.startswith('@'):\n",
    "                    h = '@' + h\n",
    "                cleaned_handles.append(h)\n",
    "        \n",
    "        return ','.join(cleaned_handles) if cleaned_handles else None\n",
    "    \n",
    "    else:\n",
    "        # Single handle\n",
    "        if handle_str == '--' or handle_str == '':\n",
    "            return None\n",
    "        \n",
    "        # Add @ if not present\n",
    "        if not handle_str.startswith('@'):\n",
    "            return '@' + handle_str\n",
    "        \n",
    "        return handle_str\n",
    "\n",
    "# Apply cleaning to Twitter Handle\n",
    "df_working['Twitter_Handle_cleaned'] = df_working['Twitter Handle'].apply(clean_twitter_handle)\n",
    "\n",
    "print(\"Twitter handle cleaning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca046e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE/AFTER COMPARISON - Twitter Handle\n",
    "print(\"=== TWITTER HANDLE BEFORE/AFTER COMPARISON ===\")\n",
    "\n",
    "# Show changes\n",
    "changes_mask = df_working['Twitter Handle'].astype(str) != df_working['Twitter_Handle_cleaned'].astype(str)\n",
    "changes_df = df_working[changes_mask][['First Name', 'Last name', 'Twitter Handle', 'Twitter_Handle_cleaned']]\n",
    "\n",
    "print(f\"Total changes made: {len(changes_df)}\")\n",
    "if len(changes_df) > 0:\n",
    "    print(\"\\nFirst 15 changes:\")\n",
    "    print(changes_df.head(15))\n",
    "else:\n",
    "    print(\"No changes needed.\")\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\n=== STATISTICS ===\")\n",
    "print(f\"Original non-null handles: {df_working['Twitter Handle'].notna().sum()}\")\n",
    "print(f\"Cleaned non-null handles: {df_working['Twitter_Handle_cleaned'].notna().sum()}\")\n",
    "\n",
    "# Check for handles without @ in original\n",
    "no_at_original = df_working['Twitter Handle'].str.contains('^[^@]', na=False, regex=True).sum()\n",
    "no_at_cleaned = df_working['Twitter_Handle_cleaned'].str.contains('^[^@]', na=False, regex=True).sum()\n",
    "print(f\"Handles without @ in original: {no_at_original}\")\n",
    "print(f\"Handles without @ in cleaned: {no_at_cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7544d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY TWITTER HANDLE CHANGES (Only run this cell if you're satisfied with the changes above)\n",
    "apply_twitter_changes = input(\"Apply Twitter Handle cleaning changes? (yes/no): \").lower().strip()\n",
    "\n",
    "if apply_twitter_changes == 'yes':\n",
    "    df_working['Twitter Handle'] = df_working['Twitter_Handle_cleaned']\n",
    "    df_working.drop('Twitter_Handle_cleaned', axis=1, inplace=True)\n",
    "    print(\"‚úÖ Twitter Handle changes applied successfully!\")\n",
    "else:\n",
    "    df_working.drop('Twitter_Handle_cleaned', axis=1, inplace=True)\n",
    "    print(\"‚ùå Twitter Handle changes discarded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08831c36",
   "metadata": {},
   "source": [
    "## Section 4: BlueSky Handles\n",
    "\n",
    "Let's clean the BlueSky handle column by:\n",
    "- Standardizing the .bsky.social domain format\n",
    "- Removing duplicates\n",
    "- Handling multiple handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine bluesky_handle column\n",
    "print(\"=== BLUESKY HANDLE ANALYSIS ===\")\n",
    "print(f\"Total entries: {len(df_working)}\")\n",
    "print(f\"Non-null entries: {df_working['bluesky_handle'].notna().sum()}\")\n",
    "print(f\"Unique entries: {df_working['bluesky_handle'].nunique()}\")\n",
    "\n",
    "print(\"\\n=== SAMPLE BLUESKY HANDLES ===\")\n",
    "bluesky_handles = df_working['bluesky_handle'].dropna().unique()[:20]\n",
    "for handle in bluesky_handles:\n",
    "    print(f\"'{handle}'\")\n",
    "\n",
    "print(\"\\n=== HANDLE FORMAT ANALYSIS ===\")\n",
    "has_bsky = df_working['bluesky_handle'].str.contains('.bsky.social', na=False).sum()\n",
    "no_bsky = df_working['bluesky_handle'].notna().sum() - has_bsky\n",
    "print(f\"Handles with .bsky.social: {has_bsky}\")\n",
    "print(f\"Handles without .bsky.social: {no_bsky}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ed118f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BlueSky Handle Cleaning Function\n",
    "def clean_bluesky_handle(handle):\n",
    "    \"\"\"Clean BlueSky handle by ensuring proper .bsky.social format\"\"\"\n",
    "    if pd.isna(handle) or handle == '' or handle == '--':\n",
    "        return None\n",
    "    \n",
    "    handle_str = str(handle).strip()\n",
    "    \n",
    "    if handle_str == '--' or handle_str == '':\n",
    "        return None\n",
    "    \n",
    "    # If it already has .bsky.social, return as is\n",
    "    if '.bsky.social' in handle_str:\n",
    "        return handle_str\n",
    "    \n",
    "    # If it doesn't have the domain, add it\n",
    "    # Remove @ if present (BlueSky doesn't use @)\n",
    "    clean_handle = handle_str.lstrip('@')\n",
    "    \n",
    "    # Add .bsky.social if not present\n",
    "    if not clean_handle.endswith('.bsky.social'):\n",
    "        return f\"{clean_handle}.bsky.social\"\n",
    "    \n",
    "    return clean_handle\n",
    "\n",
    "# Apply cleaning to bluesky_handle\n",
    "df_working['bluesky_handle_cleaned'] = df_working['bluesky_handle'].apply(clean_bluesky_handle)\n",
    "\n",
    "print(\"BlueSky handle cleaning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210b28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE/AFTER COMPARISON - BlueSky Handle\n",
    "print(\"=== BLUESKY HANDLE BEFORE/AFTER COMPARISON ===\")\n",
    "\n",
    "# Show changes\n",
    "changes_mask = df_working['bluesky_handle'].astype(str) != df_working['bluesky_handle_cleaned'].astype(str)\n",
    "changes_df = df_working[changes_mask][['First Name', 'Last name', 'bluesky_handle', 'bluesky_handle_cleaned']]\n",
    "\n",
    "print(f\"Total changes made: {len(changes_df)}\")\n",
    "if len(changes_df) > 0:\n",
    "    print(\"\\nFirst 15 changes:\")\n",
    "    print(changes_df.head(15))\n",
    "else:\n",
    "    print(\"No changes needed.\")\n",
    "\n",
    "# Show statistics\n",
    "print(\"\\n=== STATISTICS ===\")\n",
    "print(f\"Original non-null handles: {df_working['bluesky_handle'].notna().sum()}\")\n",
    "print(f\"Cleaned non-null handles: {df_working['bluesky_handle_cleaned'].notna().sum()}\")\n",
    "\n",
    "# Check format compliance\n",
    "has_bsky_original = df_working['bluesky_handle'].str.contains('.bsky.social', na=False).sum()\n",
    "has_bsky_cleaned = df_working['bluesky_handle_cleaned'].str.contains('.bsky.social', na=False).sum()\n",
    "print(f\"Handles with .bsky.social format in original: {has_bsky_original}\")\n",
    "print(f\"Handles with .bsky.social format in cleaned: {has_bsky_cleaned}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386e1455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY BLUESKY HANDLE CHANGES (Only run this cell if you're satisfied with the changes above)\n",
    "apply_bluesky_changes = input(\"Apply BlueSky Handle cleaning changes? (yes/no): \").lower().strip()\n",
    "\n",
    "if apply_bluesky_changes == 'yes':\n",
    "    df_working['bluesky_handle'] = df_working['bluesky_handle_cleaned']\n",
    "    df_working.drop('bluesky_handle_cleaned', axis=1, inplace=True)\n",
    "    print(\"‚úÖ BlueSky Handle changes applied successfully!\")\n",
    "else:\n",
    "    df_working.drop('bluesky_handle_cleaned', axis=1, inplace=True)\n",
    "    print(\"‚ùå BlueSky Handle changes discarded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357a844",
   "metadata": {},
   "source": [
    "## Section 5: Name Standardization\n",
    "\n",
    "Let's clean the name columns by:\n",
    "- Standardizing capitalization\n",
    "- Removing extra whitespace\n",
    "- Handling special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7976e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine Name columns\n",
    "print(\"=== NAME COLUMNS ANALYSIS ===\")\n",
    "print(f\"Total entries: {len(df_working)}\")\n",
    "\n",
    "for col in ['First Name', 'Last name']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Non-null entries: {df_working[col].notna().sum()}\")\n",
    "    print(f\"  Unique entries: {df_working[col].nunique()}\")\n",
    "    \n",
    "    # Show some examples of potential issues\n",
    "    sample_names = df_working[col].dropna().head(10).tolist()\n",
    "    print(f\"  Sample values: {sample_names}\")\n",
    "    \n",
    "    # Check for all lowercase or all uppercase\n",
    "    all_lower = df_working[col].str.islower().sum()\n",
    "    all_upper = df_working[col].str.isupper().sum()\n",
    "    print(f\"  All lowercase: {all_lower}\")\n",
    "    print(f\"  All uppercase: {all_upper}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5776617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name Cleaning Function\n",
    "def clean_name(name):\n",
    "    \"\"\"Clean name by standardizing capitalization and removing extra whitespace\"\"\"\n",
    "    if pd.isna(name) or name == '':\n",
    "        return name\n",
    "    \n",
    "    name_str = str(name).strip()\n",
    "    \n",
    "    # Handle special cases (hyphenated names, apostrophes, etc.)\n",
    "    # Split by spaces and title case each part\n",
    "    parts = name_str.split()\n",
    "    cleaned_parts = []\n",
    "    \n",
    "    for part in parts:\n",
    "        # Handle hyphenated names\n",
    "        if '-' in part:\n",
    "            hyphen_parts = [p.capitalize() for p in part.split('-')]\n",
    "            cleaned_parts.append('-'.join(hyphen_parts))\n",
    "        # Handle names with apostrophes (O'Connor, D'Angelo, etc.)\n",
    "        elif \"'\" in part:\n",
    "            apos_parts = part.split(\"'\")\n",
    "            if len(apos_parts) == 2:\n",
    "                cleaned_parts.append(f\"{apos_parts[0].capitalize()}'{apos_parts[1].capitalize()}\")\n",
    "            else:\n",
    "                cleaned_parts.append(part.capitalize())\n",
    "        else:\n",
    "            cleaned_parts.append(part.capitalize())\n",
    "    \n",
    "    return ' '.join(cleaned_parts)\n",
    "\n",
    "# Apply cleaning to name columns\n",
    "df_working['First_Name_cleaned'] = df_working['First Name'].apply(clean_name)\n",
    "df_working['Last_name_cleaned'] = df_working['Last name'].apply(clean_name)\n",
    "\n",
    "print(\"Name cleaning completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdab31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEFORE/AFTER COMPARISON - Names\n",
    "print(\"=== NAME COLUMNS BEFORE/AFTER COMPARISON ===\")\n",
    "\n",
    "# Show changes for First Name\n",
    "first_name_changes = df_working['First Name'].astype(str) != df_working['First_Name_cleaned'].astype(str)\n",
    "first_changes_df = df_working[first_name_changes][['First Name', 'First_Name_cleaned']]\n",
    "\n",
    "print(f\"First Name changes: {len(first_changes_df)}\")\n",
    "if len(first_changes_df) > 0:\n",
    "    print(\"\\nFirst Name changes (first 10):\")\n",
    "    print(first_changes_df.head(10))\n",
    "\n",
    "# Show changes for Last Name\n",
    "last_name_changes = df_working['Last name'].astype(str) != df_working['Last_name_cleaned'].astype(str)\n",
    "last_changes_df = df_working[last_name_changes][['Last name', 'Last_name_cleaned']]\n",
    "\n",
    "print(f\"\\nLast Name changes: {len(last_changes_df)}\")\n",
    "if len(last_changes_df) > 0:\n",
    "    print(\"\\nLast Name changes (first 10):\")\n",
    "    print(last_changes_df.head(10))\n",
    "\n",
    "print(f\"\\nTotal name records that will be updated: {(first_name_changes | last_name_changes).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e92ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APPLY NAME CHANGES (Only run this cell if you're satisfied with the changes above)\n",
    "apply_name_changes = input(\"Apply Name cleaning changes? (yes/no): \").lower().strip()\n",
    "\n",
    "if apply_name_changes == 'yes':\n",
    "    df_working['First Name'] = df_working['First_Name_cleaned']\n",
    "    df_working['Last name'] = df_working['Last_name_cleaned']\n",
    "    df_working.drop(['First_Name_cleaned', 'Last_name_cleaned'], axis=1, inplace=True)\n",
    "    print(\"‚úÖ Name changes applied successfully!\")\n",
    "else:\n",
    "    df_working.drop(['First_Name_cleaned', 'Last_name_cleaned'], axis=1, inplace=True)\n",
    "    print(\"‚ùå Name changes discarded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c963e",
   "metadata": {},
   "source": [
    "## Section 6: Final Review and Export\n",
    "\n",
    "Let's review all changes and export the cleaned dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbb857a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Data Summary\n",
    "print(\"=== FINAL CLEANED DATASET SUMMARY ===\")\n",
    "print(f\"Total records: {len(df_working)}\")\n",
    "print(f\"Total columns: {len(df_working.columns)}\")\n",
    "\n",
    "print(\"\\n=== DATA QUALITY IMPROVEMENTS ===\")\n",
    "\n",
    "# Compare original vs cleaned\n",
    "improvements = []\n",
    "\n",
    "# Email improvements\n",
    "if 'Email_1' in df_working.columns:\n",
    "    orig_emails_semicolon = df_original['Email_1'].str.endswith(';', na=False).sum()\n",
    "    clean_emails_semicolon = df_working['Email_1'].str.endswith(';', na=False).sum()\n",
    "    improvements.append(f\"Email semicolons removed: {orig_emails_semicolon - clean_emails_semicolon}\")\n",
    "\n",
    "# Phone improvements\n",
    "if 'Cellphone Number' in df_working.columns:\n",
    "    orig_phone_placeholders = (df_original['Cellphone Number'] == '--').sum()\n",
    "    clean_phone_placeholders = (df_working['Cellphone Number'] == '--').sum()\n",
    "    improvements.append(f\"Phone placeholders removed: {orig_phone_placeholders - clean_phone_placeholders}\")\n",
    "\n",
    "# Twitter improvements\n",
    "if 'Twitter Handle' in df_working.columns:\n",
    "    orig_no_at = df_original['Twitter Handle'].str.match(r'^[^@]', na=False).sum()\n",
    "    clean_no_at = df_working['Twitter Handle'].str.match(r'^[^@]', na=False).sum()\n",
    "    improvements.append(f\"Twitter handles with @ added: {orig_no_at - clean_no_at}\")\n",
    "\n",
    "# BlueSky improvements\n",
    "if 'bluesky_handle' in df_working.columns:\n",
    "    orig_no_bsky = df_original['bluesky_handle'].notna().sum() - df_original['bluesky_handle'].str.contains('.bsky.social', na=False).sum()\n",
    "    clean_no_bsky = df_working['bluesky_handle'].notna().sum() - df_working['bluesky_handle'].str.contains('.bsky.social', na=False).sum()\n",
    "    improvements.append(f\"BlueSky domains added: {orig_no_bsky - clean_no_bsky}\")\n",
    "\n",
    "for improvement in improvements:\n",
    "    print(f\"‚úÖ {improvement}\")\n",
    "\n",
    "print(\"\\n=== SAMPLE OF CLEANED DATA ===\")\n",
    "display_cols = ['First Name', 'Last name', 'Email_1', 'Cellphone Number', 'Twitter Handle', 'bluesky_handle']\n",
    "available_cols = [col for col in display_cols if col in df_working.columns]\n",
    "print(df_working[available_cols].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Cleaned Dataset\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f\"AFT_RR_GRP1_cleaned_{timestamp}.csv\"\n",
    "\n",
    "export_data = input(\"Export cleaned dataset? (yes/no): \").lower().strip()\n",
    "\n",
    "if export_data == 'yes':\n",
    "    # Save cleaned data\n",
    "    df_working.to_csv(output_filename, index=False)\n",
    "    print(f\"‚úÖ Cleaned dataset exported as: {output_filename}\")\n",
    "    \n",
    "    # Save backup of original\n",
    "    backup_filename = f\"AFT_RR_GRP1_original_backup_{timestamp}.csv\"\n",
    "    df_original.to_csv(backup_filename, index=False)\n",
    "    print(f\"‚úÖ Original dataset backed up as: {backup_filename}\")\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"   Original records: {len(df_original)}\")\n",
    "    print(f\"   Cleaned records: {len(df_working)}\")\n",
    "    print(f\"   Records preserved: 100%\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Export cancelled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8313d3",
   "metadata": {},
   "source": [
    "## Additional Cleaning Sections (Optional)\n",
    "\n",
    "The sections below can be used for additional cleaning tasks if needed. Uncomment and modify as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691133ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean Email_2 and Email_3 columns\n",
    "# Uncomment the lines below if you want to clean additional email columns\n",
    "\n",
    "# for email_col in ['Email_2', 'Email_3']:\n",
    "#     if email_col in df_working.columns:\n",
    "#         print(f\"\\n=== CLEANING {email_col} ===\")\n",
    "#         df_working[f'{email_col}_cleaned'] = df_working[email_col].apply(clean_email)\n",
    "#         \n",
    "#         # Show changes\n",
    "#         changes = df_working[email_col].astype(str) != df_working[f'{email_col}_cleaned'].astype(str)\n",
    "#         print(f\"Changes in {email_col}: {changes.sum()}\")\n",
    "#         \n",
    "#         # Apply if desired\n",
    "#         apply_changes = input(f\"Apply {email_col} changes? (yes/no): \").lower().strip()\n",
    "#         if apply_changes == 'yes':\n",
    "#             df_working[email_col] = df_working[f'{email_col}_cleaned']\n",
    "#         df_working.drop(f'{email_col}_cleaned', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d66a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Standardize State abbreviations\n",
    "# Uncomment to clean the State column\n",
    "\n",
    "# state_mapping = {\n",
    "#     'new hampshire': 'NH',\n",
    "#     'massachusetts': 'MA',\n",
    "#     'california': 'CA',\n",
    "#     'texas': 'TX',\n",
    "#     'west virginia': 'WV',\n",
    "#     'massachusetts (ma)': 'MA'\n",
    "# }\n",
    "\n",
    "# def clean_state(state):\n",
    "#     if pd.isna(state) or state == '':\n",
    "#         return state\n",
    "#     \n",
    "#     state_str = str(state).strip().lower()\n",
    "#     return state_mapping.get(state_str, str(state).strip().upper())\n",
    "\n",
    "# if 'State' in df_working.columns:\n",
    "#     df_working['State_cleaned'] = df_working['State'].apply(clean_state)\n",
    "#     print(\"State standardization completed. Review and apply if needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3e228b",
   "metadata": {},
   "source": [
    "## Data Cleaning Complete! üéâ\n",
    "\n",
    "**What was accomplished:**\n",
    "- ‚úÖ Incremental, safe data cleaning\n",
    "- ‚úÖ Before/after comparisons for all changes\n",
    "- ‚úÖ Preserved all original data\n",
    "- ‚úÖ User control over each cleaning step\n",
    "- ‚úÖ Automatic backup creation\n",
    "\n",
    "**Key improvements made:**\n",
    "- Email addresses: Removed trailing semicolons\n",
    "- Phone numbers: Standardized format, removed placeholders\n",
    "- Twitter handles: Added @ prefix where missing\n",
    "- BlueSky handles: Added .bsky.social domain\n",
    "- Names: Standardized capitalization\n",
    "\n",
    "**Next steps:**\n",
    "1. Review the exported cleaned dataset\n",
    "2. Test with a small subset of your data first\n",
    "3. Run additional cleaning sections if needed\n",
    "4. Import cleaned data into your main system\n",
    "\n",
    "**Safety features used:**\n",
    "- Original data preserved as backup\n",
    "- User confirmation required for each step\n",
    "- Visual before/after comparisons\n",
    "- Incremental processing to avoid data loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
